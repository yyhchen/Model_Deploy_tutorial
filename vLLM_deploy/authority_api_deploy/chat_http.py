import gradio as gr
import json
import requests
import vllm.entrypoints
import re


"""

æœ¬ä¾‹ä½¿ç”¨ request æ–¹æ³•å‘ API å‘é€è¯·æ±‚è·å–æ•°æ®å®ç°å¯¹è¯


ä¸‹é¢ä»¥ qwen/Qwen-7B-Chat æ¨¡å‹ä¸ºä¾‹ï¼Œè¿è¡Œæ¨¡å‹å¯¹è¯çš„æ­¥éª¤ï¼š

    å…ˆè¿è¡Œï¼šï¼ˆæ–°å»ºç»ˆç«¯è¿è¡Œï¼‰
        python -m vllm.entrypoints.openai.api_server --model qwen/Qwen-7B-Chat --dtype auto --api-key token-abc123
    
    åœ¨è¿è¡Œï¼šï¼ˆå¦æ–°å»ºç»ˆç«¯ï¼‰
        python chat_http.py
    
"""


special_chars = ['<|im_start|>', '<|im_end|>', '<|endoftext|>']

def remove_special_characters(s, special_chars):
    pattern = '|'.join(re.escape(char) for char in special_chars)
    return re.sub(pattern, '', s)


def inference(prompt, history):
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer token-abc123"
    }

    data = {
        "model": "qwen/Qwen-7B-Chat",
        "messages": [{"role": "user", "content": prompt}]
    }

    response=requests.post("http://localhost:8000/v1/chat/completions",json=data,stream=True, headers=headers)

    for chunk in response.iter_lines(chunk_size=8192,
                                     decode_unicode=False,
                                     delimiter=b"\0"):
        
        if chunk:
            data = json.loads(chunk.decode("utf-8"))
            print("====data:", data)
            output = data["choices"][0]["message"]["content"].rstrip('\r\n')
            output = remove_special_characters(output, special_chars)
            yield output

gr.ChatInterface(
    inference,
    chatbot=gr.Chatbot(height=300),
    textbox=gr.Textbox(placeholder="Chat with me!", container=False, scale=7),
    description="This is the demo for Gradio UI consuming TGI endpoint with huggingfaceğŸ¤— model.",
    title="Qwen ğŸ‡¨ğŸ‡³ vLLM ğŸš€",
    examples=["ä½ æ˜¯è°?", "ä½ èƒ½å¹²ä»€ä¹ˆï¼Ÿ", "è¯·ä½ ä»‹ç»ä¸‹åŒ—äº¬"],
    retry_btn="Retry",
    undo_btn="Undo",
    clear_btn="Clear",
).queue().launch(server_name="0.0.0.0", share=False)